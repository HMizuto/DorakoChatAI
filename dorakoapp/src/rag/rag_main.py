# -*- coding: utf-8 -*-
"""
rag_search.py - æœ€æ–° OpenAI API å¯¾å¿œç‰ˆ
-------------------------------------
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ embedding åŒ–ã—ã€
RDS(PostgreSQL + pgvector) ã‹ã‚‰é¡ä¼¼ chunk ã‚’æ¤œç´¢ã—ã€
gpt-4o-mini ã§å›ç­”ç”Ÿæˆã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import psycopg2
from pgvector.psycopg2 import register_vector
from openai import OpenAI
import config as config
from memory.conversation_store import ConversationService
from pathlib import Path
# ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿


BASE_DIR = Path(__file__).resolve().parents[1]
prompt_path = BASE_DIR / "docs" / "prompts" / "base_system_prompt.txt"

with open(prompt_path, "r", encoding="utf-8") as f:
    BASE_PROMPT = f.read()

# ----------------------------------------
# DB æ¥ç¶š
# ----------------------------------------
PG_HOST = config.PG_HOST
PG_PORT = config.PG_PORT
PG_DB = config.PG_DB
PG_USER = config.PG_USER
PG_PASSWORD = config.PG_PASSWORD


def get_conn():
    conn = psycopg2.connect(
        host=PG_HOST,
        port=PG_PORT,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASSWORD
    )
    register_vector(conn)
    return conn


# ----------------------------------------
# è³ªå•ã® embedding
# ----------------------------------------
def get_embedding(text: str):
    try:
        client = OpenAI(api_key=config.OPENAI_API_KEY)
        
        print("Embeddingå¯¾è±¡:", text)

        res = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return res.data[0].embedding

    except Exception as e:
        print("ğŸ”¥ embeddingã‚¨ãƒ©ãƒ¼:", e)
        raise

# ----------------------------------------
# é¡ä¼¼æ¤œç´¢
# ----------------------------------------
def search_similar_chunks(question, top_k=1, threshold=0.25):
    embedding = get_embedding(question)
    # embedding_str = "[" + ",".join(map(str, embedding)) + "]"

    conn = get_conn()
    cur = conn.cursor()

    print("embeddingæ¬¡å…ƒ:", len(embedding))

    cur.execute("""
        SELECT
            question,
            answer,
            embedding <=> %s::vector AS distance
        FROM qa_vectors
        ORDER BY distance
        LIMIT %s
    """, (embedding, top_k))

    rows = cur.fetchall()
    conn.close()

    print("==== ç”Ÿæ¤œç´¢çµæœ ====")
    for r in rows:
        print("distance:", r[2])
    
    results = [
            {"question": r[0], "answer": r[1], "distance": r[2]}
            for r in rows if r[2] <= threshold
    ]
    return results


# ----------------------------------------
# RAG å›ç­”ç”Ÿæˆï¼ˆæœ€æ–° APIï¼‰
# ----------------------------------------
def answer_question(user_id: str, question: str):
    # â‘  ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’ä¿å­˜
    ConversationService.save_message(user_id, "user", question)

    # â‘¡ ä¼šè©±å±¥æ­´å–å¾—
    history = ConversationService.get_recent_messages(user_id, limit=8)

    # â‘¢ RAGæ¤œç´¢
    chunks = search_similar_chunks(question)

    # â‘£ RAGãªã— â†’ é€šå¸¸ä¼šè©±
    if not chunks:
        answer = chat_with_ai_with_history(user_id, question)
        ConversationService.save_message(user_id, "assistant", answer)
        return answer

    # â‘¤ contextä½œæˆ
    context_text = "\n\n".join(
        [f"Q: {c['question']}\nA: {c['answer']}" for c in chunks]
    )

    client = OpenAI(api_key=config.OPENAI_API_KEY)

    messages = [
        {"role": "system", "content": [{"type": "input_text", "text": BASE_PROMPT}]},
        *history,
        {
            "role": "system",
            "content": [{"type": "input_text", "text": f"ä»¥ä¸‹ã¯çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚ç„¡é–¢ä¿‚ãªã‚‰ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚\n{context_text}"}]
        },
        {"role": "user", "content": [{"type": "input_text", "text": question}]}
    ]

    response = client.responses.create(
        model="gpt-4o-mini",
        input=messages,
        temperature=0.2
    )

    answer = response.output_text

    # â‘¥ AIç™ºè¨€ã‚’ä¿å­˜
    ConversationService.save_message(user_id, "assistant", answer)

    return answer



# ----------------------------------------
# Non-RAG é€šå¸¸ãƒãƒ£ãƒƒãƒˆ
# ----------------------------------------
def chat_with_ai_with_history(user_id: str, user_text: str):
    history = ConversationService.get_recent_messages(user_id)

    client = OpenAI(api_key=config.OPENAI_API_KEY)

    messages = [
        {
            "role": "system",
            "content": [{"type": "input_text", "text": BASE_PROMPT}]
        },
        *history,
        {
            "role": "user",
            "content": [{"type": "input_text", "text": user_text}]
        }
    ]

    response = client.responses.create(
        model="gpt-4o-mini",
        input=messages,
        temperature=0.7
    )

    answer = response.output_text
    return answer




# ----------------------------------------
# ãƒ†ã‚¹ãƒˆç”¨ main
# ----------------------------------------
if __name__ == "__main__":
    q = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    ans = answer_question(q)
    print("\n=== å›ç­” ===")
    print(ans)